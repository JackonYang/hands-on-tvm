{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler\n",
    "from tvm.relay import data_dep_optimization as ddo\n",
    "import tvm.relay.testing\n",
    "from tvm.contrib import graph_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(name, batch_size, layout=\"NHWC\", dtype=\"float32\", use_sparse=False):\n",
    "    \"\"\"Get the symbol definition and random weight of a network\"\"\"\n",
    "\n",
    "    # auto-scheduler prefers NHWC layout\n",
    "    if layout == \"NHWC\":\n",
    "        image_shape = (224, 224, 3)\n",
    "    elif layout == \"NCHW\":\n",
    "        image_shape = (3, 224, 224)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid layout: \" + layout)\n",
    "\n",
    "    input_shape = (batch_size,) + image_shape\n",
    "    output_shape = (batch_size, 1000)\n",
    "\n",
    "    if name.startswith(\"resnet-\"):\n",
    "        n_layer = int(name.split(\"-\")[1])\n",
    "        mod, params = relay.testing.resnet.get_workload(\n",
    "            num_layers=n_layer,\n",
    "            batch_size=batch_size,\n",
    "            layout=layout,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name.startswith(\"resnet3d-\"):\n",
    "        n_layer = int(name.split(\"-\")[1])\n",
    "        mod, params = relay.testing.resnet.get_workload(\n",
    "            num_layers=n_layer,\n",
    "            batch_size=batch_size,\n",
    "            layout=layout,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name == \"mobilenet\":\n",
    "        mod, params = relay.testing.mobilenet.get_workload(\n",
    "            batch_size=batch_size, layout=layout, dtype=dtype, image_shape=image_shape\n",
    "        )\n",
    "    elif name == \"squeezenet_v1.1\":\n",
    "        assert layout == \"NCHW\", \"squeezenet_v1.1 only supports NCHW layout\"\n",
    "        mod, params = relay.testing.squeezenet.get_workload(\n",
    "            version=\"1.1\",\n",
    "            batch_size=batch_size,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name == \"inception_v3\":\n",
    "        input_shape = (batch_size, 3, 299, 299) if layout == \"NCHW\" else (batch_size, 299, 299, 3)\n",
    "        mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)\n",
    "    elif name == \"mxnet\":\n",
    "        # an example for mxnet model\n",
    "        from mxnet.gluon.model_zoo.vision import get_model\n",
    "\n",
    "        assert layout == \"NCHW\"\n",
    "\n",
    "        block = get_model(\"resnet50_v1\", pretrained=True)\n",
    "        mod, params = relay.frontend.from_mxnet(block, shape={\"data\": input_shape}, dtype=dtype)\n",
    "        net = mod[\"main\"]\n",
    "        net = relay.Function(\n",
    "            net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs\n",
    "        )\n",
    "        mod = tvm.IRModule.from_expr(net)\n",
    "    elif name == \"mlp\":\n",
    "        mod, params = relay.testing.mlp.get_workload(\n",
    "            batch_size=batch_size, dtype=dtype, image_shape=image_shape, num_classes=1000\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Network not found.\")\n",
    "\n",
    "    if use_sparse:\n",
    "        from tvm.topi.sparse.utils import convert_model_dense_to_sparse\n",
    "\n",
    "        mod, params = convert_model_dense_to_sparse(mod, params, bs_r=4, random_params=True)\n",
    "\n",
    "    return mod, params, input_shape, output_shape\n",
    "\n",
    "\n",
    "# Define the neural network and compilation target.\n",
    "# If the target machine supports avx512 instructions, replace the\n",
    "# \"llvm -mcpu=core-avx2\" with \"llvm -mcpu=skylake-avx512\"\n",
    "network = \"resnet-50\"\n",
    "use_sparse = False\n",
    "batch_size = 1\n",
    "layout = \"NHWC\"\n",
    "target = tvm.target.Target(\"llvm -mcpu=core-avx2\")\n",
    "dtype = \"float32\"\n",
    "log_file = \"%s-%s-B%d-%s.json\" % (network, layout, batch_size, target.kind.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get model...\n",
      "Extract tasks...\n",
      "========== Task 0  (workload key: [\"6d628209072e3e3dd8f49359935acea6\", [1, 28, 28, 512], [1, 1, 512, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 512]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 512, 128]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 1  (workload key: [\"3060808fc5c74e18b1276729071fbae0\", [1, 56, 56, 64], [1, 1, 64, 256], [1, 56, 56, 256], [1, 56, 56, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 64]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 64, 256]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 56, 56, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 2  (workload key: [\"2d10de6646307f0e3e5cf4b31c20e69b\", [1, 56, 56, 64], [1, 1, 64, 256], [1, 56, 56, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 64]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 64, 256]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 3  (workload key: [\"6d628209072e3e3dd8f49359935acea6\", [1, 56, 56, 64], [1, 1, 64, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 64]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 64, 64]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 4  (workload key: [\"00a059b856ac30ac172b6252254479a6\", [1, 2048], [1000, 2048], [1, 1000], [1, 1000]]) ==========\n",
      "p0 = PLACEHOLDER [1, 2048]\n",
      "p1 = PLACEHOLDER [1000, 2048]\n",
      "T_matmul_NT(i, j) += (p0[i, k]*p1[j, k])\n",
      "p2 = PLACEHOLDER [1, 1000]\n",
      "T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + p2[ax0, ax1])\n",
      "\n",
      "========== Task 5  (workload key: [\"76afb7bf408a1ffa0b8b7bc09d077dc3\", [1, 56, 56, 64], [1, 1, 64, 256], [1, 56, 56, 256], [1, 1, 1, 256], [1, 56, 56, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 64]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 64, 256]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 56, 56, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "p3 = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + p3[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 6  (workload key: [\"0fad1b42d0d33418e0a8d15d3bbad3c9\", [1, 14, 14, 1024], [1, 1, 1024, 2048], [1, 7, 7, 2048]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 1024]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 1024, 2048]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 7  (workload key: [\"8c53ca2904398da2889aa7508082d7bb\", [1, 7, 7, 2048], [1, 1, 1, 2048]]) ==========\n",
      "p0 = PLACEHOLDER [1, 7, 7, 2048]\n",
      "adaptive_pool_sum(ax0, ax1, ax2, ax3) += p0[ax0, ((ax1*7) + rv0), ((ax2*7) + rv1), ax3]\n",
      "adaptive_pool_avg(ax0, ax1, ax2, ax3) = (adaptive_pool_sum[ax0, ax1, ax2, ax3]/(float32((select((bool)1, ((ax1 + 1)*7), (((ax1 + 1)*7) + 1)) - (ax1*7)))*float32((select((bool)1, ((ax2 + 1)*7), (((ax2 + 1)*7) + 1)) - (ax2*7)))))\n",
      "\n",
      "========== Task 8  (workload key: [\"2beb39e9afe4c74822fffbcbb8533595\", [1, 14, 14, 1024], [1, 1, 1024, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 1024]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 1024, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 9  (workload key: [\"3060808fc5c74e18b1276729071fbae0\", [1, 7, 7, 512], [1, 1, 512, 2048], [1, 7, 7, 2048], [1, 7, 7, 2048]]) ==========\n",
      "p0 = PLACEHOLDER [1, 7, 7, 512]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 512, 2048]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 7, 7, 2048]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 10  (workload key: [\"2beb39e9afe4c74822fffbcbb8533595\", [1, 56, 56, 256], [1, 1, 256, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 256]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 256, 128]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 11  (workload key: [\"0fad1b42d0d33418e0a8d15d3bbad3c9\", [1, 56, 56, 256], [1, 1, 256, 512], [1, 28, 28, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 256]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 256, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 12  (workload key: [\"76afb7bf408a1ffa0b8b7bc09d077dc3\", [1, 28, 28, 128], [1, 1, 128, 512], [1, 28, 28, 512], [1, 1, 1, 512], [1, 28, 28, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 128]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 128, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 28, 28, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "p3 = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + p3[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 13  (workload key: [\"7d79c516e212fe1d73f5dbb90eaca2cf\", [1, 1000], [1, 1000]]) ==========\n",
      "p0 = PLACEHOLDER [1, 1000]\n",
      "T_softmax_maxelem(i0) max= p0[i0, k]\n",
      "T_softmax_exp(i0, i1) = tir.exp((p0[i0, i1] - T_softmax_maxelem[i0]))\n",
      "T_softmax_expsum(i0) += T_softmax_exp[i0, k]\n",
      "T_softmax_norm(i0, i1) = (T_softmax_exp[i0, i1]/T_softmax_expsum[i0])\n",
      "\n",
      "========== Task 14  (workload key: [\"3060808fc5c74e18b1276729071fbae0\", [1, 14, 14, 256], [1, 1, 256, 1024], [1, 14, 14, 1024], [1, 14, 14, 1024]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 256]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 256, 1024]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 14, 14, 1024]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 15  (workload key: [\"6d628209072e3e3dd8f49359935acea6\", [1, 7, 7, 2048], [1, 1, 2048, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 7, 7, 2048]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 2048, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 16  (workload key: [\"6d628209072e3e3dd8f49359935acea6\", [1, 14, 14, 1024], [1, 1, 1024, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 1024]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 1024, 256]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 17  (workload key: [\"6d012ba18a086c11ee2b85c7324e16f2\", [1, 112, 112, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "p0 = PLACEHOLDER [1, 112, 112, 64]\n",
      "pad_temp(ax0, ax1, ax2, ax3) = tir.if_then_else(((((ax1 >= 1) && (ax1 < 113)) && (ax2 >= 1)) && (ax2 < 113)), p0[ax0, (ax1 - 1), (ax2 - 1), ax3], -3.40282e+38f)\n",
      "pool_max(ax0, ax1, ax2, ax3) max= pad_temp[ax0, ((ax1*2) + rv0), ((ax2*2) + rv1), ax3]\n",
      "p1 = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (pool_max[ax0, ax1, ax2, ax3] + p1[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 18  (workload key: [\"07f9fcad27bdd3233f86fe35a5185d33\", [1, 224, 224, 3], [7, 7, 3, 64], [1, 1, 1, 64], [1, 112, 112, 64]]) ==========\n",
      "p0 = PLACEHOLDER [1, 224, 224, 3]\n",
      "pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 3) && (i1 < 227)) && (i2 >= 3)) && (i2 < 227)), p0[i0, (i1 - 3), (i2 - 3), i3], 0f)\n",
      "p1 = PLACEHOLDER [7, 7, 3, 64]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 19  (workload key: [\"38552500208b25b4035682b0e93cbce3\", [1, 14, 14, 256], [6, 6, 256, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 256]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 15)) && (i2 >= 1)) && (i2 < 15)), p0[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 16), ((floormod(floordiv(p, 4), 4)*4) + eps), ((floormod(p, 4)*4) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 6) == 5)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 6) == 4)),  ..(OMITTED)..  (floormod(j, 6) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 6) == 0)), 1f, 0f))))))))))))))))))))))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "p1 = PLACEHOLDER [6, 6, 256, 256]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*p1[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 4) == 2)),  ..(OMITTED)..  6) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 4), floormod(w, 4), ((((n*4)*4) + (floordiv(h, 4)*4)) + floordiv(w, 4)), co]\n",
      "p2 = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 20  (workload key: [\"6d628209072e3e3dd8f49359935acea6\", [1, 56, 56, 256], [1, 1, 256, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 256]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 256, 64]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 21  (workload key: [\"f07e228ef5f642b386d23a62df615e7b\", [1, 7, 7, 512], [1, 1, 512, 2048], [1, 7, 7, 2048], [1, 1, 1, 2048], [1, 1, 1, 2048], [1, 7, 7, 2048]]) ==========\n",
      "p0 = PLACEHOLDER [1, 7, 7, 512]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 512, 2048]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 7, 7, 2048]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "p3 = PLACEHOLDER [1, 1, 1, 2048]\n",
      "T_multiply(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3]*p3[ax0, 0, 0, ax3])\n",
      "p4 = PLACEHOLDER [1, 1, 1, 2048]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_multiply[ax0, ax1, ax2, ax3] + p4[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 22  (workload key: [\"0fad1b42d0d33418e0a8d15d3bbad3c9\", [1, 28, 28, 512], [1, 1, 512, 1024], [1, 14, 14, 1024]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 512]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 512, 1024]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 23  (workload key: [\"3060808fc5c74e18b1276729071fbae0\", [1, 28, 28, 128], [1, 1, 128, 512], [1, 28, 28, 512], [1, 28, 28, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 128]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 128, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 28, 28, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 24  (workload key: [\"2beb39e9afe4c74822fffbcbb8533595\", [1, 28, 28, 512], [1, 1, 512, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 512]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 512, 256]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 25  (workload key: [\"76afb7bf408a1ffa0b8b7bc09d077dc3\", [1, 14, 14, 256], [1, 1, 256, 1024], [1, 14, 14, 1024], [1, 1, 1, 1024], [1, 14, 14, 1024]]) ==========\n",
      "p0 = PLACEHOLDER [1, 14, 14, 256]\n",
      "pad_temp(i0, i1, i2, i3) = p0[i0, i1, i2, i3]\n",
      "p1 = PLACEHOLDER [1, 1, 256, 1024]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 14, 14, 1024]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, ax1, ax2, ax3])\n",
      "p3 = PLACEHOLDER [1, 1, 1, 1024]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + p3[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 26  (workload key: [\"d37380659057397544e056461ea3bad3\", [1, 56, 56, 64], [3, 3, 64, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "p0 = PLACEHOLDER [1, 56, 56, 64]\n",
      "pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), p0[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "p1 = PLACEHOLDER [3, 3, 64, 64]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 27  (workload key: [\"cfd09cf1ca9e943f0ee12a18813a5c75\", [1, 28, 28, 128], [6, 6, 128, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "p0 = PLACEHOLDER [1, 28, 28, 128]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 29)) && (i2 >= 1)) && (i2 < 29)), p0[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 49), ((floormod(floordiv(p, 7), 7)*4) + eps), ((floormod(p, 7)*4) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 6) == 5)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 6) == 4)),  ..(OMITTED)..  (floormod(j, 6) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 6) == 0)), 1f, 0f))))))))))))))))))))))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "p1 = PLACEHOLDER [6, 6, 128, 128]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*p1[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 4) == 2)),  ..(OMITTED)..  6) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 4), floormod(w, 4), ((((n*7)*7) + (floordiv(h, 4)*7)) + floordiv(w, 4)), co]\n",
      "p2 = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 28  (workload key: [\"d37380659057397544e056461ea3bad3\", [1, 7, 7, 512], [3, 3, 512, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "p0 = PLACEHOLDER [1, 7, 7, 512]\n",
      "pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 8)) && (i2 >= 1)) && (i2 < 8)), p0[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "p1 = PLACEHOLDER [3, 3, 512, 512]\n",
      "conv2d_nhwc(nn, yy, xx, ff) += (pad_temp[nn, (yy + ry), (xx + rx), rc]*p1[ry, rx, rc, ff])\n",
      "p2 = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_nhwc[ax0, ax1, ax2, ax3] + p2[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract tasks from the network\n",
    "print(\"Get model...\")\n",
    "mod, params, input_shape, output_shape = get_network(\n",
    "    network,\n",
    "    batch_size,\n",
    "    layout,\n",
    "    dtype=dtype,\n",
    "    use_sparse=use_sparse,\n",
    ")\n",
    "print(\"Extract tasks...\")\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "    print(task.compute_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin tuning...\n",
      "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "|    0 |                          vm_mod_fused_nn_conv2d_add_nn_relu_5 |            - |              - |      0 |\n",
      "|    1 |                                    vm_mod_fused_nn_conv2d_add |            - |              - |      0 |\n",
      "|    2 |                                        vm_mod_fused_nn_conv2d |            - |              - |      0 |\n",
      "|    3 |                          vm_mod_fused_nn_conv2d_add_nn_relu_1 |            - |              - |      0 |\n",
      "|    4 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |\n",
      "|    5 |                        vm_mod_fused_nn_conv2d_add_add_nn_relu |            - |              - |      0 |\n",
      "|    6 |                                      vm_mod_fused_nn_conv2d_3 |            - |              - |      0 |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|    7 |                             vm_mod_fused_nn_global_avg_pool2d |            - |              - |      0 |\n",
      "|    8 |                          vm_mod_fused_nn_conv2d_add_nn_relu_8 |            - |              - |      0 |\n",
      "|    9 |                                  vm_mod_fused_nn_conv2d_add_3 |            - |              - |      0 |\n",
      "|   10 |                          vm_mod_fused_nn_conv2d_add_nn_relu_4 |            - |              - |      0 |\n",
      "|   11 |                                      vm_mod_fused_nn_conv2d_1 |            - |              - |      0 |\n",
      "|   12 |                      vm_mod_fused_nn_conv2d_add_add_nn_relu_1 |            - |              - |      0 |\n",
      "|   13 |                                       vm_mod_fused_nn_softmax |            - |              - |      0 |\n",
      "|   14 |                                  vm_mod_fused_nn_conv2d_add_2 |            - |              - |      0 |\n",
      "|   15 |                         vm_mod_fused_nn_conv2d_add_nn_relu_10 |            - |              - |      0 |\n",
      "|   16 |                          vm_mod_fused_nn_conv2d_add_nn_relu_7 |            - |              - |      0 |\n",
      "|   17 |                        vm_mod_fused_nn_max_pool2d_add_nn_relu |            - |              - |      0 |\n",
      "|   18 |                            vm_mod_fused_nn_conv2d_add_nn_relu |            - |              - |      0 |\n",
      "|   19 | vm_mod_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1 |            - |              - |      0 |\n",
      "|   20 |                          vm_mod_fused_nn_conv2d_add_nn_relu_3 |            - |              - |      0 |\n",
      "|   21 |               vm_mod_fused_nn_conv2d_add_multiply_add_nn_relu |            - |              - |      0 |\n",
      "|   22 |                                      vm_mod_fused_nn_conv2d_2 |            - |              - |      0 |\n",
      "|   23 |                                  vm_mod_fused_nn_conv2d_add_1 |            - |              - |      0 |\n",
      "|   24 |                          vm_mod_fused_nn_conv2d_add_nn_relu_6 |            - |              - |      0 |\n",
      "|   25 |                      vm_mod_fused_nn_conv2d_add_add_nn_relu_2 |            - |              - |      0 |\n",
      "|   26 |                          vm_mod_fused_nn_conv2d_add_nn_relu_2 |            - |              - |      0 |\n",
      "|   27 | vm_mod_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu |            - |              - |      0 |\n",
      "|   28 |                          vm_mod_fused_nn_conv2d_add_nn_relu_9 |            - |              - |      0 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 3\n",
      "Sample Initial Population\t#s: 2011\tfail_ct: 2\tTime elapsed: 5.88\n"
     ]
    }
   ],
   "source": [
    "def run_tuning():\n",
    "    print(\"Begin tuning...\")\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=200,  # change this to 20000 to achieve the best performance\n",
    "        runner=auto_scheduler.LocalRunner(repeat=10, enable_cpu_cache_flush=True),\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    if use_sparse:\n",
    "        from tvm.topi.sparse.utils import sparse_sketch_rules\n",
    "\n",
    "        search_policy = [\n",
    "            auto_scheduler.SketchPolicy(\n",
    "                task,\n",
    "                program_cost_model=auto_scheduler.XGBModel(),\n",
    "                init_search_callbacks=sparse_sketch_rules(),\n",
    "            )\n",
    "            for task in tasks\n",
    "        ]\n",
    "\n",
    "        tuner.tune(tune_option, search_policy=search_policy)\n",
    "    else:\n",
    "        tuner.tune(tune_option)\n",
    "\n",
    "\n",
    "# We do not run the tuning in our webpage server since it takes too long.\n",
    "# Uncomment the following line to run it by yourself.\n",
    "\n",
    "run_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
