{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run DL models with TVM\n",
    "\n",
    "官网 doc: https://tvm.apache.org/docs/how_to/compile_models/index.html\n",
    "\n",
    "茴香豆的茴有几种写法? -- 跟 TVM 编译模型的方法一样多。\n",
    "\n",
    "1. TVM build & run 模型，有多种 API 组合可选。粗读代码发现，一套 API 组合对应一套轮子。\n",
    "2. 功能基本一样，用起来也基本一样。我感觉，掌握一套即可。\n",
    "3. 我选了 `relay.build` + `tvm.contrib.graph_executor`，可以 `export_library` API 把编译后的模型保存为 `.so` 文件，方便 deploy 分发。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvm versin: 0.13.dev0\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "# from tvm.ir.module import IRModule\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print('tvm versin: %s' % tvm.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 材料准备\n",
    "\n",
    "为了方便看 IR 和 Graph，我选了一个简单的模型：`AlexNet`。\n",
    "\n",
    "识别的正确性，差一点。这不重要。只要，tvm 的识别结果与 pytorch 一致，即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model list doc: https://pytorch.org/vision/main/models.html\n",
    "model = torchvision.models.alexnet(weights='IMAGENET1K_V1')\n",
    "# model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "model = model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'test-data/cat.png'\n",
    "img = Image.open(img_path).resize((224, 224))\n",
    "\n",
    "my_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "img = my_preprocess(img)\n",
    "# 新增Batch维度\n",
    "img = np.expand_dims(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to TorchScripted model, so that we can use it in TVM\n",
    "img_input_name = 'input0'\n",
    "img_shape = [1, 3, 224, 224]\n",
    "input_shapes = [(img_input_name, img_shape)]\n",
    "\n",
    "input_data = torch.randn(img_shape)\n",
    "scripted_model = torch.jit.trace(model, input_data).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑一下 PyTorch 的分类结果作为 TVM 的标准答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch top-1 id: 285\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch_img = torch.from_numpy(img)\n",
    "    output = model(torch_img)\n",
    "\n",
    "    # Get top-1 result for PyTorch\n",
    "    top1_torch = np.argmax(output.numpy())\n",
    "    print('Torch top-1 id: %d' % top1_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TVM build & run API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 新手：\n",
    "1. IRModule 是一个桥梁。是 TVM 的一种中间表示 (IR)。\n",
    "2. 各个 DL 框架都能通过 tvm frontend 转成 IRModule。比如: pytorch, tensorflow, onnx 等。\n",
    "3. 只要转 IRModule 成功，后面的 build & run API，都是一样的。来自哪个 DL 框架，没区别。\n",
    "\n",
    "Notes:\n",
    "1. mod 是一个 IRModule。\n",
    "2. params 是一个 dict，存储了模型的所有权重信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch model to IRModule using relay(tvm frontend)\n",
    "mod, params = relay.frontend.from_pytorch(scripted_model, input_shapes)\n",
    "\n",
    "# onnx\n",
    "# mod, params = relay.frontend.from_onnx(onnx_model, input_shapes)\n",
    "# TensorFlow\n",
    "# mod, params = relay.frontend.from_tensorflow(graph_def, shape=input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some common configurations\n",
    "target = 'llvm'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 组合 1: relay.build + tvm.contrib.graph_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM top-1 id: 285\n"
     ]
    }
   ],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "\n",
    "m = graph_executor.GraphModule(lib[\"default\"](tvm.cpu(0)))\n",
    "m.set_input(img_input_name, tvm.nd.array(img.astype('float32')))\n",
    "m.run()\n",
    "tvm_output = m.get_output(0)\n",
    "\n",
    "top1_tvm_1 = np.argmax(tvm_output.numpy()[0])\n",
    "print(\"TVM top-1 id: %s\" % top1_tvm_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 组合 2: build_module + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    intrp = relay.build_module.create_executor(\"graph\", mod, tvm.cpu(0), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM top-1 id: 285\n"
     ]
    }
   ],
   "source": [
    "tvm_output2 = intrp.evaluate()(tvm.nd.array(img.astype('float32')), **params)\n",
    "\n",
    "top1_tvm_2 = np.argmax(tvm_output2.numpy()[0])\n",
    "print(\"TVM top-1 id: %s\" % top1_tvm_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 组合 3: relax.build + vm\n",
    "\n",
    "没跑通 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import relax\n",
    "\n",
    "mod_with_params = relax.transform.BindParams(\"main\", params)(mod)\n",
    "ex = relax.build(mod_with_params, target=target)\n",
    "\n",
    "# ex = relax.build(mod, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Module has no function 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vm \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mVirtualMachine(ex, tvm\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m----> 3\u001b[0m nd_res \u001b[39m=\u001b[39m vm[\u001b[39m'\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m'\u001b[39;49m](tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(img\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m      4\u001b[0m top1_tvm_3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(nd_res\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTVM top-1 id: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m top1_tvm_2)\n",
      "File \u001b[0;32m~/jksapce/learn-tvm/tvm/python/tvm/runtime/relax_vm.py:140\u001b[0m, in \u001b[0;36mVirtualMachine.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PackedFunc:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule[key]\n",
      "File \u001b[0;32m~/jksapce/learn-tvm/tvm/python/tvm/runtime/module.py:193\u001b[0m, in \u001b[0;36mModule.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, string_types):\n\u001b[1;32m    192\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only take string as function name\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_function(name)\n",
      "File \u001b[0;32m~/jksapce/learn-tvm/tvm/python/tvm/runtime/module.py:177\u001b[0m, in \u001b[0;36mModule.get_function\u001b[0;34m(self, name, query_imports)\u001b[0m\n\u001b[1;32m    171\u001b[0m check_call(\n\u001b[1;32m    172\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMModGetFunction(\n\u001b[1;32m    173\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, c_str(name), ctypes\u001b[39m.\u001b[39mc_int(query_imports), ctypes\u001b[39m.\u001b[39mbyref(ret_handle)\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ret_handle\u001b[39m.\u001b[39mvalue:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule has no function \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m PackedFunc(ret_handle, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Module has no function 'main'"
     ]
    }
   ],
   "source": [
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "nd_res = vm['main'](tvm.nd.array(img.astype('float32')))\n",
    "top1_tvm_3 = np.argmax(nd_res.numpy()[0])\n",
    "print(\"TVM top-1 id: %s\" % top1_tvm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
